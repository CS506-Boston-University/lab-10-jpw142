{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0ec0af5d",
      "cell_type": "markdown",
      "source": "Logistic Regression — Binary & Multiclass (From Scratch) + EDA",
      "metadata": {
        "id": "0ec0af5d"
      }
    },
    {
      "id": "4dc47fe1",
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer, load_iris, load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix, classification_report)\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nimport os\nplt.rcParams['figure.figsize'] = (6,4)\nnp.random.seed(42)",
      "metadata": {
        "id": "4dc47fe1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "Z-Q0F3AK5wTc",
      "cell_type": "markdown",
      "source": "\n## Math Primer (What You'll Implement)\n\n### 1) Binary Logistic Regression\n* We map $( z = \\mathbf{x}^\\top\\mathbf{w} + b $) through the sigmoid:\n$\n\\sigma(z) = \\frac{1}{1+e^{-z}}, \\qquad \\hat{y} = \\sigma(z) \\in (0,1).\n$\n<br>\n\n* The **binary cross-entropy** with L2 regularization ($(\\lambda$)) over \\(N\\) samples:\n$\n\\mathcal{L}(\\mathbf{w},b) = -\\frac{1}{N}\\sum_{i=1}^{N}\\Big[y_i\\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i)\\Big] + \\frac{\\lambda}{2N}\\|\\mathbf{w}\\|_2^2.\n$\n\n<br><br>\n**Gradients**\n$\n\\nabla_{\\mathbf{w}} = \\tfrac{1}{N}\\mathbf{X}^\\top(\\hat{\\mathbf{y}}-\\mathbf{y}) + \\tfrac{\\lambda}{N}\\mathbf{w}, \\qquad\n\\nabla_b = \\tfrac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_i-y_i).\n$\n\nWe then update parameters with gradient descent $( \\mathbf{w}\\leftarrow\\mathbf{w}-\\eta\\nabla_{\\mathbf{w}},\\; b\\leftarrow b-\\eta \\nabla_b $).\n\n\n",
      "metadata": {
        "id": "Z-Q0F3AK5wTc"
      }
    },
    {
      "id": "a233e456",
      "cell_type": "markdown",
      "source": "---\n## Part A — Breast Cancer (Binary) — EDA",
      "metadata": {
        "id": "a233e456"
      }
    },
    {
      "id": "03f33789",
      "cell_type": "markdown",
      "source": "**Goal:** Understand class balance, feature distributions, correlations, and a 2D PCA view before modeling.",
      "metadata": {
        "id": "03f33789"
      }
    },
    {
      "id": "b2b87bd6",
      "cell_type": "code",
      "source": "X_bc, y_bc = load_breast_cancer(return_X_y=True)\nfeat_bc = load_breast_cancer().feature_names\ndf_bc = pd.DataFrame(X_bc, columns=feat_bc)\ndf_bc['target'] = y_bc\nprint('Shape:', df_bc.shape)\nprint('Class counts:')\nprint(df_bc['target'].value_counts())",
      "metadata": {
        "id": "b2b87bd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74cf30a5",
      "cell_type": "code",
      "source": "plt.figure(); df_bc['target'].value_counts().sort_index().plot(kind='bar');\nplt.title('Breast Cancer — Class Counts'); plt.xlabel('Class (0=malignant, 1=benign)'); plt.ylabel('Count'); plt.show()",
      "metadata": {
        "id": "74cf30a5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "34dd245d",
      "cell_type": "code",
      "source": "for col in df_bc.columns[:10]:  # show first 10 features to avoid too many plots\n    plt.figure();\n    df_bc[df_bc['target']==0][col].plot(kind='hist', bins=30, alpha=0.5)\n    df_bc[df_bc['target']==1][col].plot(kind='hist', bins=30, alpha=0.5)\n    plt.title(f'Breast Cancer — Feature: {col}'); plt.xlabel(col); plt.ylabel('Frequency'); plt.show()",
      "metadata": {
        "id": "34dd245d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "675b2f2e",
      "cell_type": "code",
      "source": "corr = df_bc.drop(columns=['target']).corr()\nplt.figure(); plt.imshow(corr, aspect='auto'); plt.title('Breast Cancer — Correlation Heatmap'); plt.colorbar(); plt.xticks([]); plt.yticks([]); plt.show()",
      "metadata": {
        "id": "675b2f2e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ee8449cd",
      "cell_type": "code",
      "source": "scaler_tmp = StandardScaler()\nZ = scaler_tmp.fit_transform(df_bc.drop(columns=['target']))\nZ2 = PCA(n_components=2).fit_transform(Z)\nplt.figure()\nfor c in np.unique(y_bc):\n    pts = Z2[y_bc==c]\n    plt.scatter(pts[:,0], pts[:,1], s=16, label=f'class {c}')\nplt.legend(); plt.title('Breast Cancer — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()",
      "metadata": {
        "id": "ee8449cd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6291048a",
      "cell_type": "markdown",
      "source": "### Modeling Setup ",
      "metadata": {
        "id": "6291048a"
      }
    },
    {
      "id": "8be619e3",
      "cell_type": "code",
      "source": "Xtr_bc, Xva_bc, ytr_bc, yva_bc = train_test_split(X_bc, y_bc, test_size=0.25, stratify=y_bc, random_state=42)\nscaler_bc = StandardScaler()\nXtr_bc_std = scaler_bc.fit_transform(Xtr_bc)\nXva_bc_std = scaler_bc.transform(Xva_bc)",
      "metadata": {
        "id": "8be619e3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a245350",
      "cell_type": "code",
      "source": "def sigmoid(z):\n    # ------------------------------------------------\n    # TODO Refer to the Binary Logistic Regression Sigmoid Function defined in the Math Primer (Exercise 1)\n    # You may want to use np.exp\n    # ------------------------------------------------\n    return 1.0 / (1.0 + np.exp(-z))\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n",
      "metadata": {
        "id": "2a245350"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ed0da7e",
      "cell_type": "code",
      "source": "def bin_loss_and_grads(X, y, w, b, lam):\n    N = X.shape[0]\n    z = X @ w + b\n    p = sigmoid(z)\n    eps = 1e-12\n\n    # ------------------------------------------------\n    # TODO Refer to the Binary Cross-Entropy Loss Function defined in the Math Primer (Exercise 2)\n    # You may want to use np.exp\n    # ------------------------------------------------\n    ce = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n    reg = (lam/(2.0*N))*(w@w)\n    loss = ce + reg\n    gw = (X.T @ (p - y))/N + (lam/N)*w\n    gb = np.mean(p - y)\n    return loss, gw, gb\n",
      "metadata": {
        "id": "1ed0da7e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e49f687d",
      "cell_type": "code",
      "source": "def fit_logreg_binary(X, y, lr=0.1, lam=0.01, epochs=2000, batch_size=None, verbose=True):\n    N, D = X.shape; w = np.zeros(D); b = 0.0; losses = []\n    for t in range(1, epochs+1):\n        if batch_size is None:\n            Xb, yb = X, y\n        else:\n            idx = np.random.choice(N, size=batch_size, replace=False); Xb, yb = X[idx], y[idx]\n        loss, gw, gb = bin_loss_and_grads(Xb, yb, w, b, lam)\n        w -= lr*gw; b -= lr*gb\n        L,_,_ = bin_loss_and_grads(X, y, w, b, lam); losses.append(L)\n        if verbose and (t % max(1, epochs//10) == 0):\n            pass\n    return w, b, np.array(losses)\n",
      "metadata": {
        "id": "e49f687d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "303f2a60",
      "cell_type": "code",
      "source": "w_bc, b_bc, losses_bc = fit_logreg_binary(Xtr_bc_std, ytr_bc)\nif len(losses_bc)>0:\n    plt.figure(); plt.plot(losses_bc); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Breast Cancer: Training Loss'); plt.show()",
      "metadata": {
        "id": "303f2a60"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9b9d7c8",
      "cell_type": "code",
      "source": "def predict_proba_binary(X, w, b):\n    # ------------------------------------------------\n    # TODO Refer to the Binary Logistic Regression Sigmoid Function defined in the Math Primer (Exercise 3)\n    # What is the Z defined as\n    # ------------------------------------------------\n    return sigmoid(X @ w + b)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n",
      "metadata": {
        "id": "b9b9d7c8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "511437e8",
      "cell_type": "code",
      "source": "def predict_label_binary(X, w, b, thresh=0.5):\n    # ------------------------------------------------\n    # TODO We want to determine the binary label of the data if it is above a certain threshold (Exercise 4)\n    # You may want to return it as a type int\n    # ------------------------------------------------\n    return (predict_proba_binary(X, w, b) >= thresh).astype(int)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n",
      "metadata": {
        "id": "511437e8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "62be4195",
      "cell_type": "code",
      "source": "p_bc = predict_proba_binary(Xva_bc_std, w_bc, b_bc)\nyhat_bc = predict_label_binary(Xva_bc_std, w_bc, b_bc)\nprint('Breast Cancer — Acc/Prec/Rec/F1/AUC:',\n      round(accuracy_score(yva_bc, yhat_bc),4),\n      round(precision_score(yva_bc, yhat_bc),4),\n      round(recall_score(yva_bc, yhat_bc),4),\n      round(f1_score(yva_bc, yhat_bc),4),\n      round(roc_auc_score(yva_bc, p_bc),4))\nfpr, tpr, _ = roc_curve(yva_bc, p_bc)\nplt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC — Breast Cancer'); plt.show()\ncm = confusion_matrix(yva_bc, yhat_bc)\nprint(cm)\nplt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Breast Cancer'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()",
      "metadata": {
        "id": "62be4195"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "UFKGMxRLyiHY",
      "cell_type": "code",
      "source": "# ------------------------------------------------\n# TODO We will be utilizing the LogisticRegression model from sklearn, we will use a max_iter=10000 (Exercise 5)\n# First we will train on our data Xtr_bc_std\n# After we want to do predictions on the validation set data Xva_bc_std\n# ------------------------------------------------\nclf_bc = LogisticRegression(max_iter=10000)\nclf_bc.fit(Xtr_bc_std, ytr_bc)\npred_bc = clf_bc.predict(Xva_bc_std)\n\n# ----------------------------\n# Implementation Ends Here\n# ----------------------------\n\nprob_bc = clf_bc.predict_proba(Xva_bc_std)[:,1]\nprint('sklearn — Acc/Prec/Rec/F1/AUC:',\n      round(accuracy_score(yva_bc, pred_bc),4),\n      round(precision_score(yva_bc, pred_bc),4),\n      round(recall_score(yva_bc, pred_bc),4),\n      round(f1_score(yva_bc, pred_bc),4),\n      round(roc_auc_score(yva_bc, prob_bc),4))",
      "metadata": {
        "id": "UFKGMxRLyiHY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f6de2cf0",
      "cell_type": "markdown",
      "source": "---\n## Part B — Titanic (Binary) — EDA",
      "metadata": {
        "id": "f6de2cf0"
      }
    },
    {
      "id": "7a92cb5d",
      "cell_type": "markdown",
      "source": "**Goal:** Inspect missingness and the relationship between survival and key variables.",
      "metadata": {
        "id": "7a92cb5d"
      }
    },
    {
      "id": "bb04fd1d",
      "cell_type": "code",
      "source": "TRAIN_CSV = 'train.csv'\nTEST_CSV  = 'test.csv'\nif not os.path.exists(TRAIN_CSV):\n    print('Place Kaggle Titanic train.csv beside the notebook to run this section.')\nelse:\n    titanic = pd.read_csv(TRAIN_CSV)\n    print('Shape:', titanic.shape)\n    print(titanic.isna().sum())",
      "metadata": {
        "id": "bb04fd1d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2154a049",
      "cell_type": "code",
      "source": "if os.path.exists(TRAIN_CSV):\n    plt.figure(); titanic['Survived'].value_counts().sort_index().plot(kind='bar');\n    plt.title('Titanic — Class Counts'); plt.xlabel('Survived'); plt.ylabel('Count'); plt.show()\n    plt.figure(); titanic['Age'].dropna().plot(kind='hist', bins=30); plt.title('Age Distribution'); plt.xlabel('Age'); plt.show()\n    plt.figure(); titanic['Fare'].dropna().plot(kind='hist', bins=30); plt.title('Fare Distribution'); plt.xlabel('Fare'); plt.show()\n    plt.figure(); titanic.groupby('Sex')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Sex'); plt.ylabel('Rate'); plt.show()\n    plt.figure(); titanic.groupby('Pclass')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Pclass'); plt.ylabel('Rate'); plt.show()\n    if titanic['Embarked'].notna().any():\n        plt.figure(); titanic.groupby('Embarked')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Embarked'); plt.ylabel('Rate'); plt.show()",
      "metadata": {
        "id": "2154a049"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6f470a6f",
      "cell_type": "code",
      "source": "if os.path.exists(TRAIN_CSV):\n    # Basic numeric PCA view\n    num_cols = ['Pclass','Age','SibSp','Parch','Fare']\n    tmp = titanic[num_cols].copy()\n    tmp = tmp.fillna(tmp.median())\n    Z = StandardScaler().fit_transform(tmp)\n    Z2 = PCA(n_components=2).fit_transform(Z)\n    y = titanic['Survived'].values\n    plt.figure()\n    for c in np.unique(y):\n        pts = Z2[y==c]\n        plt.scatter(pts[:,0], pts[:,1], s=14, label=f'Survived={c}')\n    plt.legend(); plt.title('Titanic — PCA (numeric features)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()",
      "metadata": {
        "id": "6f470a6f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d92a1c5",
      "cell_type": "markdown",
      "source": "### Titanic — Modeling (reuse binary functions)",
      "metadata": {
        "id": "8d92a1c5"
      }
    },
    {
      "id": "a2be2187",
      "cell_type": "code",
      "source": "if os.path.exists(TRAIN_CSV):\n    df = titanic.copy()\n    df['Age'] = df['Age'].fillna(df['Age'].median())\n    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n    df['Sex'] = df['Sex'].map({'male':0,'female':1})\n    df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n    features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_Q','Embarked_S']\n    X_ti = df[features]\n    y_ti = df['Survived']\n    Xtr_ti, Xva_ti, ytr_ti, yva_ti = train_test_split(X_ti, y_ti, test_size=0.25, stratify=y_ti, random_state=42)\n    scaler_ti = StandardScaler()\n    Xtr_ti_std = scaler_ti.fit_transform(Xtr_ti)\n    Xva_ti_std = scaler_ti.transform(Xva_ti)\n\n    # ------------------------------------------------\n    # TODO We will utilize the fit_logreg_binary we defined in the first part, we will be using the Xtr_ti_std as the train data (Exercise 6)\n    # lr=0.1, lam=0.01, epochs=1500 and verbose=False\n    # ------------------------------------------------\n    w_ti, b_ti, losses_ti = fit_logreg_binary(\n    Xtr_ti_std,                 # training features\n    ytr_ti.to_numpy(),          # training labels\n    lr=0.1,\n    lam=0.01,\n    epochs=1500,\n    verbose=False\n)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n\n    plt.figure(); plt.plot(losses_ti); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Titanic: Training Loss'); plt.show()\n    p_ti = predict_proba_binary(Xva_ti_std, w_ti, b_ti)\n    yhat_ti = predict_label_binary(Xva_ti_std, w_ti, b_ti)\n    print('Titanic Acc/Prec/Rec/F1/AUC:', round(accuracy_score(yva_ti, yhat_ti),4), round(precision_score(yva_ti, yhat_ti),4), round(recall_score(yva_ti, yhat_ti),4), round(f1_score(yva_ti, yhat_ti),4), round(roc_auc_score(yva_ti, p_ti),4))\n\n    clf_ti = LogisticRegression(max_iter=10000)\n    clf_ti.fit(Xtr_ti_std, ytr_ti)\n    pred_ti = clf_ti.predict(Xva_ti_std)\n    prob_ti = clf_ti.predict_proba(Xva_ti_std)[:,1]\n    print('sklearn (Titanic):', round(accuracy_score(yva_ti, pred_ti),4), round(precision_score(yva_ti, pred_ti),4), round(recall_score(yva_ti, pred_ti),4), round(f1_score(yva_ti, pred_ti),4), round(roc_auc_score(yva_ti, prob_ti),4))",
      "metadata": {
        "id": "a2be2187"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fb246c2b",
      "cell_type": "markdown",
      "source": "---\n## Part C — Iris (Multiclass) — EDA",
      "metadata": {
        "id": "fb246c2b"
      }
    },
    {
      "id": "3gxHaIM-6es4",
      "cell_type": "markdown",
      "source": "### 2) Multiclass Logistic Regression (Softmax)\nFor \\(K\\) classes, with $(\\mathbf{W}\\in\\mathbb{R}^{D\\times K}$), $(\\mathbf{b}\\in\\mathbb{R}^{K}$):\n$\n\\mathbf{z} = \\mathbf{W}^\\top \\mathbf{x} + \\mathbf{b},\\qquad\n\\hat{p}_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}} = \\text{softmax}(\\mathbf{z})_k.\n$\n\n<br><br>\nUse one-hot labels $( \\mathbf{Y}\\in\\{0,1\\}^{N\\times K} $) and predictions $( \\hat{\\mathbf{P}}\\in[0,1]^{N\\times K} $):\n\n**Total Loss Function**: $\n\\mathcal{L}(\\mathbf{W},\\mathbf{b}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K Y_{ik}\\log \\hat{P}_{ik} + \\frac{\\lambda}{2N}\\|\\mathbf{W}\\|_F^2.\n$\n\n<br><br>\n**Gradients:**\n$\n\\nabla_{\\mathbf{W}} = \\tfrac{1}{N}\\mathbf{X}^\\top(\\hat{\\mathbf{P}}-\\mathbf{Y}) + \\tfrac{\\lambda}{N}\\mathbf{W}, \\qquad\n\\nabla_{\\mathbf{b}} = \\tfrac{1}{N}\\sum_{i=1}^{N}(\\hat{\\mathbf{P}}_{i\\cdot}-\\mathbf{Y}_{i\\cdot}).\n$\n",
      "metadata": {
        "id": "3gxHaIM-6es4"
      }
    },
    {
      "id": "2091a1a0",
      "cell_type": "markdown",
      "source": "**Goal:** Visualize distributions and separability across the 3 classes.",
      "metadata": {
        "id": "2091a1a0"
      }
    },
    {
      "id": "51d72e9e",
      "cell_type": "code",
      "source": "X_ir, y_ir = load_iris(return_X_y=True)\nfeat_ir = load_iris().feature_names\ndf_ir = pd.DataFrame(X_ir, columns=feat_ir)\ndf_ir['target'] = y_ir\nprint('Class counts:', df_ir['target'].value_counts().sort_index().to_dict())",
      "metadata": {
        "id": "51d72e9e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "46957daa",
      "cell_type": "code",
      "source": "for col in feat_ir:\n    plt.figure();\n    for c in np.unique(y_ir):\n        df_ir[df_ir['target']==c][col].plot(kind='hist', bins=25, alpha=0.5)\n    plt.title(f'Iris — Feature: {col}'); plt.xlabel(col); plt.ylabel('Frequency'); plt.show()",
      "metadata": {
        "id": "46957daa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1daa66f7",
      "cell_type": "code",
      "source": "scaler_tmp = StandardScaler()\nZ = scaler_tmp.fit_transform(df_ir.drop(columns=['target']))\nZ2 = PCA(n_components=2).fit_transform(Z)\nplt.figure()\nfor c in np.unique(y_ir):\n    pts = Z2[y_ir==c]\n    plt.scatter(pts[:,0], pts[:,1], s=16, label=f'class {c}')\nplt.legend(); plt.title('Iris — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()",
      "metadata": {
        "id": "1daa66f7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e50f96ff",
      "cell_type": "markdown",
      "source": "### Iris — Modeling (Softmax)",
      "metadata": {
        "id": "e50f96ff"
      }
    },
    {
      "id": "34e4cbdd",
      "cell_type": "code",
      "source": "Xtr_ir, Xva_ir, ytr_ir, yva_ir = train_test_split(X_ir, y_ir, test_size=0.25, stratify=y_ir, random_state=42)\nscaler_ir = StandardScaler()\nXtr_ir_std = scaler_ir.fit_transform(Xtr_ir)\nXva_ir_std = scaler_ir.transform(Xva_ir)",
      "metadata": {
        "id": "34e4cbdd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6c1114c5",
      "cell_type": "code",
      "source": "def one_hot(y, K):\n    Y = np.zeros((y.shape[0], K)); Y[np.arange(y.shape[0]), y] = 1.0; return Y\n",
      "metadata": {
        "id": "6c1114c5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d3f1a0e",
      "cell_type": "code",
      "source": "def softmax(Z):\n    Zs = Z - Z.max(axis=1, keepdims=True)\n    # ------------------------------------------------\n    # TODO Refer to the Multiclass Logistic Regression Softmax section above (Exercise 7)\n    # You may want to use np.exp\n    # ------------------------------------------------\n    expZ = np.exp(Zs)\n    return expZ / expZ.sum(axis=1, keepdims=True)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n",
      "metadata": {
        "id": "3d3f1a0e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ddb952ea",
      "cell_type": "code",
      "source": "def softmax_loss_and_grads(X, Y, W, b, lam):\n    N = X.shape[0]\n    Z = X @ W + b\n    P = softmax(Z)\n    eps = 1e-12\n    # ------------------------------------------------\n    # TODO Refer to the Multiclass Logistic Regression Total Loss function defined above (Exercise 8)\n    # You may want to use np.log\n    # ------------------------------------------------\n    ce = -np.sum(Y * np.log(P + eps)) / N\n    reg = (lam / (2.0 * N)) * np.sum(W * W)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n\n    loss = ce + reg\n    dZ = (P - Y)/N\n    dW = X.T @ dZ + (lam/N)*W\n    db = dZ.sum(axis=0)\n    return loss, dW, db\n",
      "metadata": {
        "id": "ddb952ea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "44edad2d",
      "cell_type": "code",
      "source": "def fit_softmax(X, y, lr=0.1, lam=0.01, epochs=1500, batch_size=None, verbose=True):\n    N, D = X.shape; K=int(y.max()+1); Y=one_hot(y,K); W=np.zeros((D,K)); b=np.zeros(K); losses=[]\n    for t in range(1, epochs+1):\n        if batch_size is None:\n            Xb, Yb = X, Y\n        else:\n            idx = np.random.choice(N, size=batch_size, replace=False); Xb, Yb = X[idx], Y[idx]\n        loss, dW, db = softmax_loss_and_grads(Xb, Yb, W, b, lam)\n        W -= lr*dW; b -= lr*db\n        L,_,_ = softmax_loss_and_grads(X, Y, W, b, lam); losses.append(L)\n        if verbose and (t % max(1, epochs//10)==0):\n            pass\n    return W, b, np.array(losses)\n",
      "metadata": {
        "id": "44edad2d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "612e4f7a",
      "cell_type": "code",
      "source": "W_ir, b_ir, losses_ir = fit_softmax(Xtr_ir_std, ytr_ir, lr=0.1, lam=0.01, epochs=1500, verbose=True)\nif len(losses_ir)>0:\n    plt.figure(); plt.plot(losses_ir); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Iris: Training Loss'); plt.show()",
      "metadata": {
        "id": "612e4f7a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6e249c3e",
      "cell_type": "code",
      "source": "def predict_proba_softmax(X, W, b):\n    # TODO Refer to the Multiclass Logistic Regression Softmax section above (Exercise 9)\n    # Hint what is Z defined as\n    return softmax(X @ W + b)\n    # ----------------------------\n    # Implementation Ends Here\n    # ----------------------------\n",
      "metadata": {
        "id": "6e249c3e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9256743",
      "cell_type": "code",
      "source": "def predict_label_softmax(X, W, b):\n    return np.argmax(predict_proba_softmax(X, W, b), axis=1)\n",
      "metadata": {
        "id": "b9256743"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "40f4b783",
      "cell_type": "code",
      "source": "yp_ir = predict_label_softmax(Xva_ir_std, W_ir, b_ir)\nprint('Iris accuracy:', round(accuracy_score(yva_ir, yp_ir),4))\ncm = confusion_matrix(yva_ir, yp_ir); print(cm)\nplt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Iris'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()",
      "metadata": {
        "id": "40f4b783"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f3VYCBY_y-KZ",
      "cell_type": "code",
      "source": "# ------------------------------------------------\n# TODO We will be utilizing the LogisticRegression model from sklearn, we will use multinomial as the multi_class, lbfgs as the solver and the max_iter=2000 (Exercise 10)\n# First we will train on our data Xtr_ir_std\n# After we want to do predictions on the validation set data Xva_ir_std\n# ------------------------------------------------\nclf_ir = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000\n)\nclf_ir.fit(Xtr_ir_std, ytr_ir)\npred_ir = clf_ir.predict(Xva_ir_std)\n# ----------------------------\n# Implementation Ends Here\n# ----------------------------\n\nprint('sklearn (Iris) accuracy:', round(accuracy_score(yva_ir, pred_ir),4))\nprint(classification_report(yva_ir, pred_ir, digits=4))",
      "metadata": {
        "id": "f3VYCBY_y-KZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "54b44b28",
      "cell_type": "markdown",
      "source": "---\n## Part D — Digits (Multiclass) — EDA",
      "metadata": {
        "id": "54b44b28"
      }
    },
    {
      "id": "d1f78151",
      "cell_type": "markdown",
      "source": "**Goal:** See sample digits, per-class averages, and separability in PCA.",
      "metadata": {
        "id": "d1f78151"
      }
    },
    {
      "id": "d2d849da",
      "cell_type": "code",
      "source": "X_dig, y_dig = load_digits(return_X_y=True)\nprint('Digits shape:', X_dig.shape)\nimgs = X_dig.reshape(-1, 8, 8)\nfor i in range(12):\n    plt.figure(); plt.imshow(imgs[i], cmap=None); plt.title(f'Sample digit: {y_dig[i]}'); plt.axis('off'); plt.show()",
      "metadata": {
        "id": "d2d849da"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dade2df4",
      "cell_type": "code",
      "source": "avg_imgs = []\nfor k in np.unique(y_dig):\n    avg_imgs.append(imgs[y_dig==k].mean(axis=0))\navg_imgs = np.array(avg_imgs)\n",
      "metadata": {
        "id": "dade2df4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7ada6210",
      "cell_type": "code",
      "source": "for k in range(avg_imgs.shape[0]):\n    plt.figure(); plt.imshow(avg_imgs[k], cmap=None); plt.title(f'Average image for class {k}'); plt.axis('off'); plt.show()",
      "metadata": {
        "id": "7ada6210"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e578e0bd",
      "cell_type": "code",
      "source": "Z = StandardScaler().fit_transform(X_dig)\nZ2 = PCA(n_components=2).fit_transform(Z)\nplt.figure()\nfor c in np.unique(y_dig):\n    pts = Z2[y_dig==c]\n    plt.scatter(pts[:,0], pts[:,1], s=12, label=str(c))\nplt.legend(); plt.title('Digits — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()",
      "metadata": {
        "id": "e578e0bd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a0d6a3f9",
      "cell_type": "markdown",
      "source": "### Digits — Modeling (Softmax)",
      "metadata": {
        "id": "a0d6a3f9"
      }
    },
    {
      "id": "216d83b5",
      "cell_type": "code",
      "source": "Xtr_d, Xva_d, ytr_d, yva_d = train_test_split(X_dig, y_dig, test_size=0.25, stratify=y_dig, random_state=42)\nscaler_d = StandardScaler()\nXtr_d_std = scaler_d.fit_transform(Xtr_d)\nXva_d_std = scaler_d.transform(Xva_d)\n\n# ------------------------------------------------\n# TODO We will utilize the fit_softmax we defined in the first part, we will be using the Xtr_d_std as the train data (Exercise 11)\n# lr=0.2, lam=0.001, epochs=1500 and verbose=False\n# ------------------------------------------------\nW_d, b_d, losses_d = fit_softmax(\n    Xtr_d_std,\n    ytr_d,\n    lr=0.2,\n    lam=0.001,\n    epochs=1500,\n    verbose=False\n)\n# ----------------------------\n# Implementation Ends Here\n# ----------------------------\n\nif len(losses_d)>0:\n    plt.figure(); plt.plot(losses_d); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Digits: Training Loss'); plt.show()\nyp_d = predict_label_softmax(Xva_d_std, W_d, b_d)\nprint('Digits accuracy (from scratch):', round(accuracy_score(yva_d, yp_d),4))\ncm = confusion_matrix(yva_d, yp_d); print(cm)\nplt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Digits'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()",
      "metadata": {
        "id": "216d83b5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "oArd_LzjzG06",
      "cell_type": "code",
      "source": "\n# ------------------------------------------------\n# TODO We will be utilizing the LogisticRegression model from sklearn, we will use multinomial as the multi_class, lbfgs as the solver and the max_iter=2000 (Exercise 12)\n# First we will train on our data Xtr_d_std\n# After we want to do predictions on the validation set data Xva_d_std\n# ------------------------------------------------\n\nclf_d = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000\n)\nclf_d.fit(Xtr_d_std, ytr_d)\npreds_skl_d = clf_d.predict(Xva_d_std)\n# ----------------------------\n# Implementation Ends Here\n# ----------------------------\n\nprint('sklearn (Digits) accuracy:', round(accuracy_score(yva_d, preds_skl_d),4))",
      "metadata": {
        "id": "oArd_LzjzG06"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "tyn4S0wK7Xje",
      "cell_type": "markdown",
      "source": "# **Questions and Answers** (10 points)\n\n\n---\n\n### **1. Sigmoid vs Softmax**  \n\nWhy do we use the sigmoid or softmax functions instead of a linear output?\n\n**Answer:**\nBecause sigmoid and softmax convert raw scores into probabilities between 0 and 1 (or sum to 1 for multiclass).\nLinear outputs can be negative, unbounded, and not interpretable as probabilities, which breaks classification.\n\n### **2. Separating Data** \n\nWhy can Logistic Regression handle linearly separable data well, but struggle with non-linear data?\n\n**Answer:**\nBecause logistic regression uses a linear decision boundary (a straight line or hyperplane).\nIf the classes can be separated with a single line, it works well.\nIf the pattern is nonlinear (circles, spirals, XOR), no single straight line can split the classes.\n\n### **3. Logistic vs Linear** \n\nCompare Logistic Regression with Linear Regression. In what cases can both models produce similar predictions?\n\n**Answer:**\nIf labels are 0/1 and data is far from the decision boundary, linear regression might produce predictions close to 0 and 1.\nIn extremely simple, nearly linearly separable datasets, both models may behave similarly.\n\nBut logistic regression is the correct choice because it outputs bounded probabilities.\n### **4. Discriminative Model** \n\nExplain why Logistic Regression is a discriminative model and not a generative one.\n\n**Answer:**\nBecause it models P(y | x) — the probability of a class given the input.\nIt does not model how the data is generated (P(x | y)), unlike generative models such as Naive Bayes.",
      "metadata": {
        "id": "tyn4S0wK7Xje"
      }
    },
    {
      "id": "15db9db4-a606-431e-a9eb-ca6c3b90f5ae",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "K0bNKELR7YFK",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "K0bNKELR7YFK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5a5682b4-d795-462f-84ea-fa30fde4ec6e",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aff1979a-5287-4d28-b9b4-6765eab90dea",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eb8ed75d-4aef-4500-ba51-395aa65dbdd3",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4692b303-5fc5-4487-a962-4506d11351bc",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}